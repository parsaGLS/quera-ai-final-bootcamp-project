{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e9BLCjfPSt7e"
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers==4.44.2 datasets accelerate scikit-learn beautifulsoup4 emoji\n",
    "import gdown\n",
    "import pandas as pd, numpy as np, re, emoji\n",
    "from sklearn.model_selection import train_test_split\n",
    "from bs4 import BeautifulSoup\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zDJ9qlUNS90p",
    "outputId": "5611fc62-1c2e-49cb-bd3b-395f060dc631"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1-AlW7oNJHaqi3xk_9dWHUS52Dzl_FmFW\n",
      "From (redirected): https://drive.google.com/uc?id=1-AlW7oNJHaqi3xk_9dWHUS52Dzl_FmFW&confirm=t&uuid=287051bf-99c0-40cc-ae5e-f2ee5062bae2\n",
      "To: /content/train.csv\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 635M/635M [00:02<00:00, 223MB/s]\n",
      "/tmp/ipython-input-3579292728.py:4: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(train_output_path)\n"
     ]
    }
   ],
   "source": [
    "train_file_id = '1-AlW7oNJHaqi3xk_9dWHUS52Dzl_FmFW'\n",
    "train_output_path = 'train.csv'\n",
    "gdown.download(f'https://drive.google.com/uc?id={train_file_id}', train_output_path, quiet=False)\n",
    "df = pd.read_csv(train_output_path)\n",
    "df['label'] = df['overall'] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cse6HOGdTd2v"
   },
   "outputs": [],
   "source": [
    "target_per_class = 25000\n",
    "balanced_df = pd.concat([\n",
    "    df[df['label'] == cls].sample(n=target_per_class, random_state=42)\n",
    "    if len(df[df['label'] == cls]) >= target_per_class else df[df['label'] == cls]\n",
    "    for cls in sorted(df['label'].unique())\n",
    "]).sample(frac=1, random_state=42)\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    text = emoji.demojize(text, delimiters=(\" \", \" \"))\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
    "    text = re.sub(r'[^A-Za-z0-9\\s.,!?]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip().lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E95Q_o7Ocyi5"
   },
   "outputs": [],
   "source": [
    "balanced_df['cleaned_review'] = balanced_df['reviewText'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H4REvRTgTgS9"
   },
   "outputs": [],
   "source": [
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    balanced_df['cleaned_review'], balanced_df['label'],\n",
    "    test_size=0.2, stratify=balanced_df['label'], random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136,
     "referenced_widgets": [
      "d1f325362ce44f90b8f34051068c22f0",
      "3cd13ca550a04daab7d45ba311af3313",
      "00eed3e0f16b439e8c6c894be0263ebe",
      "38ae2d9c7d6b4f2ba7f213029c451ea1",
      "55bca7bfb8704f9a9bddf36c6aed4f05",
      "fac70efd7f294f6e8ea0fe3e92beff00",
      "31cfa59b026d4689a88f0c5b95e04d05",
      "65f2bf5a722d4d8996a086994411ce8a",
      "024244267fbf49089a0ede0cad3fa090",
      "eac7ded7e6e6433992e5932a94c94e4a",
      "427bec3641ce4eceba8e689659860430",
      "ffb41ab6c51a4139ab7d527198209eba",
      "fafe51425df4464b844a94b626a506ca",
      "8310465293f54876813c7b15296a573f",
      "e4aee0772e84410290773c5a08bdbfa7",
      "a80834800b754a38ab1d1f2f72efdd2f",
      "a7bab4d6423a45579e4b699e11556f0f",
      "9818703135e641cfbbb596a58c9f7c2d",
      "ad2d17b64c6d4f3ba3bd45e535730148",
      "73006521df88427d8195972db91e273b",
      "b5a27945ed8f4bb38c8fbdbb7ccad7fd",
      "2b07642d6e114437b4b7abdd31f90d7d"
     ]
    },
    "id": "kEcpoA3YTktd",
    "outputId": "7fb18734-c202-4ed5-9f28-87398fa9c0b8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1f325362ce44f90b8f34051068c22f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffb41ab6c51a4139ab7d527198209eba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding=\"max_length\", truncation=True, max_length=160)\n",
    "\n",
    "train_ds = Dataset.from_dict({'text': train_texts.tolist(), 'label': train_labels.tolist()})\n",
    "val_ds = Dataset.from_dict({'text': val_texts.tolist(), 'label': val_labels.tolist()})\n",
    "train_ds = train_ds.map(tokenize, batched=True)\n",
    "val_ds = val_ds.map(tokenize, batched=True)\n",
    "train_ds.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "val_ds.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RyGySQlssjth"
   },
   "outputs": [],
   "source": [
    "class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b4bXmZ0oTnO-",
    "outputId": "62060789-5b78-4f3e-e9e4-fc9e07024b2e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QbB9v4D9spSX"
   },
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**{k: v for k, v in inputs.items() if k != \"labels\"})\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=class_weights_tensor.to(model.device))\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wyjEwsHkTnyw"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    return {\n",
    "        'accuracy': accuracy_score(labels, preds),\n",
    "        'f1_macro': f1_score(labels, preds, average='macro')\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 276
    },
    "id": "7bh-uSSiTpkp",
    "outputId": "2a99e26d-9f37-4e42-ed90-9fa7d5601f6a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 1 ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9375' max='9375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9375/9375 09:07, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.323500</td>\n",
       "      <td>1.236004</td>\n",
       "      <td>0.460480</td>\n",
       "      <td>0.440159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.246500</td>\n",
       "      <td>1.216619</td>\n",
       "      <td>0.469880</td>\n",
       "      <td>0.461123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.236900</td>\n",
       "      <td>1.212089</td>\n",
       "      <td>0.471320</td>\n",
       "      <td>0.463120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9375, training_loss=1.2689383072916667, metrics={'train_runtime': 547.8638, 'train_samples_per_second': 547.581, 'train_steps_per_second': 17.112, 'total_flos': 1.241948304e+16, 'train_loss': 1.2689383072916667, 'epoch': 3.0})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for param in model.distilbert.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "training_args_stage1 = TrainingArguments(\n",
    "    output_dir=\"./results_stage1\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=64,\n",
    "    fp16=True,\n",
    "    logging_dir=\"./logs_stage1\",\n",
    "    logging_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "trainer_stage1 = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args_stage1,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(\"--- Stage 1 ---\")\n",
    "trainer_stage1.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 276
    },
    "id": "K_aLClweTssD",
    "outputId": "d6a0cab5-c367-4fb8-b3ba-8c2a7a6e2f26"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 2 ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9375' max='9375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9375/9375 27:43, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.039200</td>\n",
       "      <td>0.941771</td>\n",
       "      <td>0.594920</td>\n",
       "      <td>0.585355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.862800</td>\n",
       "      <td>0.904926</td>\n",
       "      <td>0.613680</td>\n",
       "      <td>0.613690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.757400</td>\n",
       "      <td>0.934443</td>\n",
       "      <td>0.613960</td>\n",
       "      <td>0.613909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9375, training_loss=0.8864675520833334, metrics={'train_runtime': 1663.9755, 'train_samples_per_second': 180.291, 'train_steps_per_second': 5.634, 'total_flos': 1.241948304e+16, 'train_loss': 0.8864675520833334, 'epoch': 3.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for param in model.distilbert.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "training_args_stage2 = TrainingArguments(\n",
    "    output_dir=\"./results_stage2\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=2,\n",
    "    per_device_eval_batch_size=32,\n",
    "    fp16=True,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs_stage2\",\n",
    "    logging_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "trainer_stage2 = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args_stage2,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(\"--- Stage 2 ---\")\n",
    "trainer_stage2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "id": "xpNwbeXmTymK",
    "outputId": "d2f86c5f-86dd-4286-b6db-b079981bcfd2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.6137\n",
      "Final F1-macro: 0.6137\n",
      "Confusion Matrix:\n",
      " [[3427 1181  270   57   65]\n",
      " [1139 2440 1211  147   63]\n",
      " [ 303  983 2793  794  127]\n",
      " [  54  117  977 2894  958]\n",
      " [  52   33  144  983 3788]]\n"
     ]
    }
   ],
   "source": [
    "preds = trainer_stage2.predict(val_ds)\n",
    "y_true = preds.label_ids\n",
    "y_pred = np.argmax(preds.predictions, axis=1)\n",
    "print(f\"Final Accuracy: {accuracy_score(y_true, y_pred):.4f}\")\n",
    "print(f\"Final F1-macro: {f1_score(y_true, y_pred, average='macro'):.4f}\")\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
